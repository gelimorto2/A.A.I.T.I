const logger = require('../utils/logger');

/**
 * ML Model Performance Tracking System for A.A.I.T.I
 * 
 * Provides comprehensive model performance monitoring including:
 * - Accuracy tracking over time
 * - Feature importance analysis
 * - Model drift detection
 * - A/B testing framework for model versions
 * - Automated retraining triggers
 * - Performance comparison dashboards
 */
class MLPerformanceTracker {
  constructor(database, mlService) {
    this.db = database;
    this.mlService = mlService;
    
    // Performance tracking configuration
    this.config = {
      // Accuracy thresholds
      minAccuracyThreshold: 0.65,      // Minimum acceptable accuracy
      accuracyWarningThreshold: 0.70,   // Warning threshold
      accuracyDeclineThreshold: 0.05,   // Max allowed accuracy decline
      
      // Drift detection
      driftDetectionWindow: 1000,       // Number of predictions to analyze
      driftThreshold: 0.1,              // Statistical drift threshold
      featureDriftThreshold: 0.15,      // Feature-level drift threshold
      
      // A/B testing
      abTestMinSampleSize: 500,         // Minimum samples for A/B test
      abTestConfidenceLevel: 0.95,      // Statistical confidence level
      abTestDuration: 7 * 24 * 60 * 60 * 1000, // 7 days in milliseconds
      
      // Retraining triggers
      retrainingAccuracyThreshold: 0.60, // Trigger retraining below this
      retrainingDriftThreshold: 0.2,     // Trigger retraining above this drift
      retrainingSchedule: 7 * 24 * 60 * 60 * 1000, // Weekly retraining
      
      // Data retention
      performanceHistoryDays: 90,       // Keep 90 days of performance data
      predictionHistoryDays: 30,        // Keep 30 days of predictions
      
      // Monitoring intervals
      performanceCheckInterval: 60 * 60 * 1000,    // Check every hour
      driftCheckInterval: 6 * 60 * 60 * 1000,      // Check drift every 6 hours
      retrainingCheckInterval: 24 * 60 * 60 * 1000  // Check retraining daily
    };
    
    // Performance state
    this.performanceState = {
      models: new Map(),               // Model ID -> performance data
      activePredictions: new Map(),    // Prediction ID -> prediction data
      driftMetrics: new Map(),         // Model ID -> drift metrics
      abTests: new Map(),              // Test ID -> A/B test data
      retrainingQueue: new Set(),      // Models pending retraining
      lastPerformanceCheck: Date.now(),
      lastDriftCheck: Date.now(),
      lastRetrainingCheck: Date.now()
    };
    
    this.initializePerformanceTracker();
  }

  async initializePerformanceTracker() {
    try {
      await this.loadModelPerformanceHistory();
      await this.initializeActivePredictions();
      this.startPerformanceMonitoring();
      
      logger.info('ðŸ“Š ML Performance Tracker initialized', {
        modelsTracked: this.performanceState.models.size,
        activePredictions: this.performanceState.activePredictions.size,
        service: 'ml-performance-tracker'
      });
    } catch (error) {
      logger.error('Failed to initialize ML performance tracker', {
        error: error.message,
        service: 'ml-performance-tracker'
      });
      throw error;
    }
  }

  /**
   * Record a model prediction for later performance tracking
   */
  async recordPrediction(modelId, predictionData) {
    try {
      const predictionId = `${modelId}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      
      const prediction = {
        id: predictionId,
        modelId,
        timestamp: Date.now(),
        input: predictionData.input,
        prediction: predictionData.prediction,
        confidence: predictionData.confidence,
        features: predictionData.features,
        metadata: predictionData.metadata || {},
        outcome: null,  // Will be filled when actual result is known
        accuracy: null,
        error: null
      };

      // Store in memory for quick access
      this.performanceState.activePredictions.set(predictionId, prediction);
      
      // Store in database for persistence
      await this.storePredictionInDatabase(prediction);
      
      logger.debug('Prediction recorded for performance tracking', {
        predictionId,
        modelId,
        confidence: predictionData.confidence,
        service: 'ml-performance-tracker'
      });

      return predictionId;
    } catch (error) {
      logger.error('Failed to record prediction', {
        error: error.message,
        modelId,
        service: 'ml-performance-tracker'
      });
      throw error;
    }
  }

  /**
   * Update prediction with actual outcome for accuracy calculation
   */
  async updatePredictionOutcome(predictionId, actualOutcome, metadata = {}) {
    try {
      const prediction = this.performanceState.activePredictions.get(predictionId);
      
      if (!prediction) {
        // Try to load from database
        const dbPrediction = await this.loadPredictionFromDatabase(predictionId);
        if (!dbPrediction) {
          throw new Error(`Prediction ${predictionId} not found`);
        }
        this.performanceState.activePredictions.set(predictionId, dbPrediction);
        prediction = dbPrediction;
      }

      // Calculate accuracy based on prediction type
      const accuracy = this.calculateAccuracy(prediction.prediction, actualOutcome, prediction.metadata.predictionType);
      const error = Math.abs(prediction.prediction - actualOutcome);

      // Update prediction
      prediction.outcome = actualOutcome;
      prediction.accuracy = accuracy;
      prediction.error = error;
      prediction.outcomeTimestamp = Date.now();
      prediction.outcomeMetadata = metadata;

      // Update in database
      await this.updatePredictionInDatabase(prediction);

      // Update model performance metrics
      await this.updateModelPerformance(prediction.modelId, prediction);

      // Check for drift
      await this.checkModelDrift(prediction.modelId);

      logger.debug('Prediction outcome updated', {
        predictionId,
        modelId: prediction.modelId,
        accuracy,
        error,
        service: 'ml-performance-tracker'
      });

      return {
        predictionId,
        accuracy,
        error,
        confidence: prediction.confidence
      };
    } catch (error) {
      logger.error('Failed to update prediction outcome', {
        error: error.message,
        predictionId,
        service: 'ml-performance-tracker'
      });
      throw error;
    }
  }

  /**
   * Calculate accuracy based on prediction type
   */
  calculateAccuracy(prediction, actual, predictionType = 'regression') {
    switch (predictionType) {
      case 'classification':
        return prediction === actual ? 1.0 : 0.0;
      
      case 'binary':
        return prediction === actual ? 1.0 : 0.0;
      
      case 'regression':
        // For regression, use relative accuracy
        if (actual === 0) return prediction === 0 ? 1.0 : 0.0;
        const relativeError = Math.abs((prediction - actual) / actual);
        return Math.max(0, 1 - relativeError);
      
      case 'direction':
        // For directional predictions (up/down)
        const predDirection = prediction > 0 ? 1 : -1;
        const actualDirection = actual > 0 ? 1 : -1;
        return predDirection === actualDirection ? 1.0 : 0.0;
      
      default:
        logger.warn('Unknown prediction type, using regression accuracy', {
          predictionType,
          service: 'ml-performance-tracker'
        });
        return this.calculateAccuracy(prediction, actual, 'regression');
    }
  }

  /**
   * Update model performance metrics
   */
  async updateModelPerformance(modelId, prediction) {
    try {
      let modelPerf = this.performanceState.models.get(modelId);
      
      if (!modelPerf) {
        modelPerf = {
          modelId,
          totalPredictions: 0,
          accuratePredictions: 0,
          totalError: 0,
          recentPredictions: [],
          performanceHistory: [],
          featureImportance: new Map(),
          lastUpdated: Date.now(),
          version: 1,
          status: 'active'
        };
        this.performanceState.models.set(modelId, modelPerf);
      }

      // Update counters
      modelPerf.totalPredictions++;
      if (prediction.accuracy > 0.5) {
        modelPerf.accuratePredictions++;
      }
      modelPerf.totalError += prediction.error;

      // Add to recent predictions (sliding window)
      modelPerf.recentPredictions.push({
        timestamp: prediction.timestamp,
        accuracy: prediction.accuracy,
        error: prediction.error,
        confidence: prediction.confidence
      });

      // Keep only recent predictions for drift detection
      const windowSize = this.config.driftDetectionWindow;
      if (modelPerf.recentPredictions.length > windowSize) {
        modelPerf.recentPredictions = modelPerf.recentPredictions.slice(-windowSize);
      }

      // Calculate current metrics
      const currentAccuracy = modelPerf.accuratePredictions / modelPerf.totalPredictions;
      const currentError = modelPerf.totalError / modelPerf.totalPredictions;
      const recentAccuracy = this.calculateRecentAccuracy(modelPerf.recentPredictions);

      // Update performance history
      const now = Date.now();
      const lastHistoryEntry = modelPerf.performanceHistory[modelPerf.performanceHistory.length - 1];
      
      if (!lastHistoryEntry || now - lastHistoryEntry.timestamp > 60 * 60 * 1000) { // Hourly snapshots
        modelPerf.performanceHistory.push({
          timestamp: now,
          accuracy: currentAccuracy,
          recentAccuracy,
          error: currentError,
          totalPredictions: modelPerf.totalPredictions,
          confidenceAvg: this.calculateAverageConfidence(modelPerf.recentPredictions)
        });
      }

      // Update feature importance if available
      if (prediction.features) {
        this.updateFeatureImportance(modelPerf, prediction);
      }

      modelPerf.lastUpdated = now;

      // Check for performance alerts
      await this.checkPerformanceAlerts(modelId, modelPerf, currentAccuracy, recentAccuracy);

      logger.debug('Model performance updated', {
        modelId,
        currentAccuracy,
        recentAccuracy,
        totalPredictions: modelPerf.totalPredictions,
        service: 'ml-performance-tracker'
      });
    } catch (error) {
      logger.error('Failed to update model performance', {
        error: error.message,
        modelId,
        service: 'ml-performance-tracker'
      });
    }
  }

  /**
   * Check for model drift
   */
  async checkModelDrift(modelId) {
    try {
      const modelPerf = this.performanceState.models.get(modelId);
      if (!modelPerf || modelPerf.recentPredictions.length < 100) {
        return; // Need sufficient data for drift detection
      }

      const driftMetrics = this.calculateDriftMetrics(modelPerf);
      this.performanceState.driftMetrics.set(modelId, driftMetrics);

      // Check for significant drift
      if (driftMetrics.overallDrift > this.config.driftThreshold) {
        logger.warn('Model drift detected', {
          modelId,
          driftScore: driftMetrics.overallDrift,
          threshold: this.config.driftThreshold,
          service: 'ml-performance-tracker'
        });

        // Trigger retraining if drift is severe
        if (driftMetrics.overallDrift > this.config.retrainingDriftThreshold) {
          this.triggerRetraining(modelId, 'drift_detected', {
            driftScore: driftMetrics.overallDrift,
            driftDetails: driftMetrics
          });
        }
      }

      return driftMetrics;
    } catch (error) {
      logger.error('Drift detection failed', {
        error: error.message,
        modelId,
        service: 'ml-performance-tracker'
      });
    }
  }

  /**
   * Calculate drift metrics
   */
  calculateDriftMetrics(modelPerf) {
    const recentPredictions = modelPerf.recentPredictions;
    const halfPoint = Math.floor(recentPredictions.length / 2);
    
    const firstHalf = recentPredictions.slice(0, halfPoint);
    const secondHalf = recentPredictions.slice(halfPoint);

    // Calculate statistical measures for both halves
    const firstHalfStats = this.calculateStatistics(firstHalf.map(p => p.accuracy));
    const secondHalfStats = this.calculateStatistics(secondHalf.map(p => p.accuracy));

    // Calculate drift using Kolmogorov-Smirnov-like test
    const meanDrift = Math.abs(firstHalfStats.mean - secondHalfStats.mean);
    const stdDrift = Math.abs(firstHalfStats.std - secondHalfStats.std);
    
    // Normalized drift score
    const overallDrift = (meanDrift + stdDrift) / 2;

    // Confidence drift
    const firstHalfConfidence = this.calculateStatistics(firstHalf.map(p => p.confidence));
    const secondHalfConfidence = this.calculateStatistics(secondHalf.map(p => p.confidence));
    const confidenceDrift = Math.abs(firstHalfConfidence.mean - secondHalfConfidence.mean);

    return {
      overallDrift,
      meanDrift,
      stdDrift,
      confidenceDrift,
      firstHalfAccuracy: firstHalfStats.mean,
      secondHalfAccuracy: secondHalfStats.mean,
      sampleSize: recentPredictions.length,
      calculatedAt: Date.now()
    };
  }

  /**
   * Calculate statistics for a dataset
   */
  calculateStatistics(data) {
    if (data.length === 0) return { mean: 0, std: 0, min: 0, max: 0 };
    
    const mean = data.reduce((sum, val) => sum + val, 0) / data.length;
    const variance = data.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / data.length;
    const std = Math.sqrt(variance);
    const min = Math.min(...data);
    const max = Math.max(...data);
    
    return { mean, std, variance, min, max };
  }

  /**
   * Start A/B test for model versions
   */
  async startABTest(modelAId, modelBId, testConfig = {}) {
    try {
      const testId = `ab_${modelAId}_${modelBId}_${Date.now()}`;
      
      const abTest = {
        id: testId,
        modelA: {
          id: modelAId,
          predictions: 0,
          accuracy: 0,
          totalAccuracy: 0,
          errors: []
        },
        modelB: {
          id: modelBId,
          predictions: 0,
          accuracy: 0,
          totalAccuracy: 0,
          errors: []
        },
        config: {
          minSampleSize: testConfig.minSampleSize || this.config.abTestMinSampleSize,
          confidenceLevel: testConfig.confidenceLevel || this.config.abTestConfidenceLevel,
          duration: testConfig.duration || this.config.abTestDuration,
          trafficSplit: testConfig.trafficSplit || 0.5
        },
        status: 'running',
        startTime: Date.now(),
        endTime: null,
        results: null,
        metadata: testConfig.metadata || {}
      };

      this.performanceState.abTests.set(testId, abTest);
      
      logger.info('A/B test started', {
        testId,
        modelA: modelAId,
        modelB: modelBId,
        duration: abTest.config.duration,
        service: 'ml-performance-tracker'
      });

      return testId;
    } catch (error) {
      logger.error('Failed to start A/B test', {
        error: error.message,
        modelAId,
        modelBId,
        service: 'ml-performance-tracker'
      });
      throw error;
    }
  }

  /**
   * Record A/B test prediction result
   */
  async recordABTestResult(testId, modelId, accuracy, error) {
    try {
      const abTest = this.performanceState.abTests.get(testId);
      if (!abTest || abTest.status !== 'running') {
        return;
      }

      const modelData = abTest.modelA.id === modelId ? abTest.modelA : abTest.modelB;
      if (!modelData) {
        return;
      }

      modelData.predictions++;
      modelData.totalAccuracy += accuracy;
      modelData.accuracy = modelData.totalAccuracy / modelData.predictions;
      modelData.errors.push(error);

      // Check if test should end
      await this.checkABTestCompletion(testId);
    } catch (error) {
      logger.error('Failed to record A/B test result', {
        error: error.message,
        testId,
        modelId,
        service: 'ml-performance-tracker'
      });
    }
  }

  /**
   * Check if A/B test is complete and analyze results
   */
  async checkABTestCompletion(testId) {
    try {
      const abTest = this.performanceState.abTests.get(testId);
      if (!abTest || abTest.status !== 'running') {
        return;
      }

      const now = Date.now();
      const hasMinSamples = abTest.modelA.predictions >= abTest.config.minSampleSize && 
                           abTest.modelB.predictions >= abTest.config.minSampleSize;
      const hasTimedOut = now - abTest.startTime >= abTest.config.duration;

      if (hasMinSamples || hasTimedOut) {
        abTest.status = 'completed';
        abTest.endTime = now;
        abTest.results = await this.analyzeABTestResults(abTest);

        logger.info('A/B test completed', {
          testId,
          duration: abTest.endTime - abTest.startTime,
          winner: abTest.results.winner,
          confidence: abTest.results.confidence,
          service: 'ml-performance-tracker'
        });

        // Auto-promote winner if significant difference
        if (abTest.results.isSignificant) {
          await this.promoteABTestWinner(testId);
        }
      }
    } catch (error) {
      logger.error('Failed to check A/B test completion', {
        error: error.message,
        testId,
        service: 'ml-performance-tracker'
      });
    }
  }

  /**
   * Analyze A/B test results with statistical significance
   */
  async analyzeABTestResults(abTest) {
    const { modelA, modelB } = abTest;
    
    // Calculate t-test for statistical significance
    const tTestResult = this.performTTest(
      modelA.errors,
      modelB.errors,
      abTest.config.confidenceLevel
    );

    const accuracyDiff = Math.abs(modelA.accuracy - modelB.accuracy);
    const winner = modelA.accuracy > modelB.accuracy ? 'A' : 'B';
    const winnerModel = winner === 'A' ? modelA : modelB;
    const loserModel = winner === 'A' ? modelB : modelA;

    return {
      winner,
      winnerModelId: winnerModel.id,
      loserModelId: loserModel.id,
      accuracyDifference: accuracyDiff,
      winnerAccuracy: winnerModel.accuracy,
      loserAccuracy: loserModel.accuracy,
      isSignificant: tTestResult.isSignificant,
      confidence: tTestResult.confidence,
      pValue: tTestResult.pValue,
      sampleSizeA: modelA.predictions,
      sampleSizeB: modelB.predictions,
      testDuration: abTest.endTime - abTest.startTime
    };
  }

  /**
   * Perform t-test for statistical significance
   */
  performTTest(samplesA, samplesB, confidenceLevel) {
    if (samplesA.length < 2 || samplesB.length < 2) {
      return { isSignificant: false, confidence: 0, pValue: 1 };
    }

    const statsA = this.calculateStatistics(samplesA);
    const statsB = this.calculateStatistics(samplesB);

    // Pooled standard error
    const pooledSE = Math.sqrt(
      (statsA.variance / samplesA.length) + 
      (statsB.variance / samplesB.length)
    );

    if (pooledSE === 0) {
      return { isSignificant: false, confidence: 0, pValue: 1 };
    }

    // t-statistic
    const tStat = Math.abs(statsA.mean - statsB.mean) / pooledSE;
    
    // Degrees of freedom
    const df = samplesA.length + samplesB.length - 2;
    
    // Simplified p-value calculation (for demonstration)
    // In production, use a proper statistical library
    const pValue = this.calculatePValue(tStat, df);
    const alpha = 1 - confidenceLevel;
    
    return {
      isSignificant: pValue < alpha,
      confidence: 1 - pValue,
      pValue,
      tStatistic: tStat,
      degreesOfFreedom: df
    };
  }

  /**
   * Simplified p-value calculation
   */
  calculatePValue(tStat, df) {
    // This is a simplified approximation
    // In production, use a proper statistical library like jStat
    if (df < 1) return 1;
    
    const absT = Math.abs(tStat);
    if (absT < 1) return 0.4;
    if (absT < 2) return 0.1;
    if (absT < 3) return 0.01;
    return 0.001;
  }

  /**
   * Trigger model retraining
   */
  async triggerRetraining(modelId, reason, metadata = {}) {
    try {
      if (this.performanceState.retrainingQueue.has(modelId)) {
        logger.debug('Model already queued for retraining', {
          modelId,
          reason,
          service: 'ml-performance-tracker'
        });
        return;
      }

      this.performanceState.retrainingQueue.add(modelId);
      
      const retrainingRequest = {
        modelId,
        reason,
        timestamp: Date.now(),
        metadata,
        status: 'queued'
      };

      // Store retraining request
      await this.storeRetrainingRequest(retrainingRequest);

      logger.info('Model retraining triggered', {
        modelId,
        reason,
        queueSize: this.performanceState.retrainingQueue.size,
        service: 'ml-performance-tracker'
      });

      // Notify ML service about retraining need
      if (this.mlService && this.mlService.scheduleRetraining) {
        await this.mlService.scheduleRetraining(modelId, reason, metadata);
      }

      return retrainingRequest;
    } catch (error) {
      logger.error('Failed to trigger retraining', {
        error: error.message,
        modelId,
        reason,
        service: 'ml-performance-tracker'
      });
    }
  }

  /**
   * Get comprehensive performance report for a model
   */
  getModelPerformanceReport(modelId) {
    const modelPerf = this.performanceState.models.get(modelId);
    if (!modelPerf) {
      return null;
    }

    const driftMetrics = this.performanceState.driftMetrics.get(modelId);
    const currentAccuracy = modelPerf.accuratePredictions / modelPerf.totalPredictions;
    const recentAccuracy = this.calculateRecentAccuracy(modelPerf.recentPredictions);

    return {
      modelId,
      version: modelPerf.version,
      status: modelPerf.status,
      performance: {
        totalPredictions: modelPerf.totalPredictions,
        accuratePredictions: modelPerf.accuratePredictions,
        currentAccuracy,
        recentAccuracy,
        averageError: modelPerf.totalError / modelPerf.totalPredictions,
        averageConfidence: this.calculateAverageConfidence(modelPerf.recentPredictions)
      },
      drift: driftMetrics || null,
      featureImportance: Object.fromEntries(modelPerf.featureImportance),
      performanceHistory: modelPerf.performanceHistory.slice(-100), // Last 100 data points
      lastUpdated: modelPerf.lastUpdated,
      needsRetraining: this.performanceState.retrainingQueue.has(modelId),
      alerts: this.getModelAlerts(modelId)
    };
  }

  /**
   * Get all active A/B tests
   */
  getActiveABTests() {
    const activeTests = Array.from(this.performanceState.abTests.values())
      .filter(test => test.status === 'running')
      .map(test => ({
        id: test.id,
        modelA: test.modelA.id,
        modelB: test.modelB.id,
        progress: {
          modelA: {
            predictions: test.modelA.predictions,
            accuracy: test.modelA.accuracy
          },
          modelB: {
            predictions: test.modelB.predictions,
            accuracy: test.modelB.accuracy
          }
        },
        startTime: test.startTime,
        remainingTime: Math.max(0, (test.startTime + test.config.duration) - Date.now())
      }));

    return activeTests;
  }

  /**
   * Start performance monitoring
   */
  startPerformanceMonitoring() {
    // Performance checks
    setInterval(async () => {
      try {
        await this.performPerformanceChecks();
      } catch (error) {
        logger.error('Performance check failed', {
          error: error.message,
          service: 'ml-performance-tracker'
        });
      }
    }, this.config.performanceCheckInterval);

    // Drift checks
    setInterval(async () => {
      try {
        await this.performDriftChecks();
      } catch (error) {
        logger.error('Drift check failed', {
          error: error.message,
          service: 'ml-performance-tracker'
        });
      }
    }, this.config.driftCheckInterval);

    // Retraining checks
    setInterval(async () => {
      try {
        await this.performRetrainingChecks();
      } catch (error) {
        logger.error('Retraining check failed', {
          error: error.message,
          service: 'ml-performance-tracker'
        });
      }
    }, this.config.retrainingCheckInterval);

    logger.info('ðŸ”„ ML Performance monitoring started', {
      performanceInterval: this.config.performanceCheckInterval,
      driftInterval: this.config.driftCheckInterval,
      retrainingInterval: this.config.retrainingCheckInterval,
      service: 'ml-performance-tracker'
    });
  }

  // Utility methods
  calculateRecentAccuracy(recentPredictions, windowSize = 100) {
    if (recentPredictions.length === 0) return 0;
    
    const window = recentPredictions.slice(-windowSize);
    const accurateCount = window.filter(p => p.accuracy > 0.5).length;
    return accurateCount / window.length;
  }

  calculateAverageConfidence(predictions) {
    if (predictions.length === 0) return 0;
    
    const totalConfidence = predictions.reduce((sum, p) => sum + p.confidence, 0);
    return totalConfidence / predictions.length;
  }

  updateFeatureImportance(modelPerf, prediction) {
    if (!prediction.features || typeof prediction.features !== 'object') {
      return;
    }

    Object.entries(prediction.features).forEach(([feature, value]) => {
      if (!modelPerf.featureImportance.has(feature)) {
        modelPerf.featureImportance.set(feature, {
          totalImportance: 0,
          count: 0,
          avgImportance: 0
        });
      }
      
      const featureData = modelPerf.featureImportance.get(feature);
      featureData.totalImportance += Math.abs(value);
      featureData.count++;
      featureData.avgImportance = featureData.totalImportance / featureData.count;
    });
  }

  async checkPerformanceAlerts(modelId, modelPerf, currentAccuracy, recentAccuracy) {
    const alerts = [];

    // Low accuracy alert
    if (currentAccuracy < this.config.minAccuracyThreshold) {
      alerts.push({
        type: 'low_accuracy',
        severity: 'critical',
        message: `Model accuracy below minimum threshold: ${(currentAccuracy * 100).toFixed(2)}%`
      });
    } else if (currentAccuracy < this.config.accuracyWarningThreshold) {
      alerts.push({
        type: 'accuracy_warning',
        severity: 'warning',
        message: `Model accuracy below warning threshold: ${(currentAccuracy * 100).toFixed(2)}%`
      });
    }

    // Accuracy decline alert
    if (modelPerf.performanceHistory.length > 1) {
      const previousAccuracy = modelPerf.performanceHistory[modelPerf.performanceHistory.length - 2].accuracy;
      const accuracyDecline = previousAccuracy - currentAccuracy;
      
      if (accuracyDecline > this.config.accuracyDeclineThreshold) {
        alerts.push({
          type: 'accuracy_decline',
          severity: 'high',
          message: `Significant accuracy decline: ${(accuracyDecline * 100).toFixed(2)}%`
        });
      }
    }

    // Low recent accuracy
    if (recentAccuracy < currentAccuracy - 0.1) {
      alerts.push({
        type: 'recent_performance_drop',
        severity: 'medium',
        message: `Recent performance drop: ${(currentAccuracy * 100).toFixed(2)}% â†’ ${(recentAccuracy * 100).toFixed(2)}%`
      });
    }

    // Log alerts
    alerts.forEach(alert => {
      logger.warn(`ðŸ“Š ML Performance Alert: ${alert.message}`, {
        modelId,
        type: alert.type,
        severity: alert.severity,
        service: 'ml-performance-tracker'
      });
    });

    return alerts;
  }

  getModelAlerts(modelId) {
    // This would typically fetch from an alerts store
    // For now, return empty array
    return [];
  }

  // Database operations (stubs - implement based on your database schema)
  async storePredictionInDatabase(prediction) {
    // Implementation depends on your database schema
    logger.debug('Storing prediction in database', { id: prediction.id });
  }

  async updatePredictionInDatabase(prediction) {
    // Implementation depends on your database schema
    logger.debug('Updating prediction in database', { id: prediction.id });
  }

  async loadPredictionFromDatabase(predictionId) {
    // Implementation depends on your database schema
    logger.debug('Loading prediction from database', { id: predictionId });
    return null;
  }

  async storeRetrainingRequest(request) {
    // Implementation depends on your database schema
    logger.debug('Storing retraining request', { modelId: request.modelId });
  }

  async loadModelPerformanceHistory() {
    // Load existing performance data from database
    logger.debug('Loading model performance history');
  }

  async initializeActivePredictions() {
    // Load recent predictions from database
    logger.debug('Initializing active predictions');
  }

  async performPerformanceChecks() {
    this.performanceState.lastPerformanceCheck = Date.now();
    // Implement periodic performance checks
  }

  async performDriftChecks() {
    this.performanceState.lastDriftCheck = Date.now();
    // Implement periodic drift checks
  }

  async performRetrainingChecks() {
    this.performanceState.lastRetrainingCheck = Date.now();
    // Implement periodic retraining checks
  }

  async promoteABTestWinner(testId) {
    const abTest = this.performanceState.abTests.get(testId);
    if (!abTest || !abTest.results) return;

    logger.info('Promoting A/B test winner', {
      testId,
      winner: abTest.results.winnerModelId,
      service: 'ml-performance-tracker'
    });

    // Implementation depends on your model deployment system
  }
}

module.exports = MLPerformanceTracker;